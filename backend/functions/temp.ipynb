{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, TargetEncoder\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, precision_recall_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "project_path = os.getenv('PROJECT_PATH')\n",
    "sys.path.append(project_path)\n",
    "sys.path.append('../Enums/')\n",
    "# from Enums.enums import ClassificationMetrics\n",
    "from enum import Enum\n",
    "class ClassificationMetrics(Enum):\n",
    "    Accuracy = \"Accuracy\"\n",
    "    AUC = \"AUC\"\n",
    "    Precision = \"Precision\"\n",
    "    Recall = \"Recall\"\n",
    "    F1 = \"F1\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ClassificationUtility():\n",
    "    def __init__(self, data, target_column, hyperparameters=None, metric_to_optimize=ClassificationMetrics.Accuracy.value):\n",
    "        self.data = data\n",
    "        self.target_column = target_column\n",
    "        self.metric_to_optimize = metric_to_optimize\n",
    "        self.cardinality_threshold = 10\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.classifiers = [\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            DecisionTreeClassifier(),\n",
    "            RandomForestClassifier(),\n",
    "            AdaBoostClassifier(),\n",
    "            SVC(kernel='linear', max_iter=1000, probability=True),\n",
    "            GaussianNB(),\n",
    "            KNeighborsClassifier()\n",
    "        ]\n",
    "        self.classifiers_dict = {\n",
    "            LogisticRegression().__class__.__name__ : LogisticRegression(max_iter=1000),\n",
    "            DecisionTreeClassifier().__class__.__name__ : DecisionTreeClassifier(),\n",
    "            RandomForestClassifier().__class__.__name__ : RandomForestClassifier(),\n",
    "            AdaBoostClassifier().__class__.__name__ : AdaBoostClassifier(),\n",
    "            SVC().__class__.__name__ : SVC(kernel='linear', max_iter=1000, probability=True),\n",
    "            GaussianNB().__class__.__name__ : GaussianNB(),\n",
    "            KNeighborsClassifier().__class__.__name__ : KNeighborsClassifier()\n",
    "        }\n",
    "\n",
    "    def get_numerical_columns(self):\n",
    "        numerical_columns = []\n",
    "        for column in self.data.columns:\n",
    "            if column != self.target_column and (self.data[column].dtype == 'int64' or self.data[column].dtype == 'float64'):\n",
    "                numerical_columns.append(column)\n",
    "        self.numerical_columns = numerical_columns\n",
    "        # return numerical_columns\n",
    "    \n",
    "    def get_categorical_columns(self):\n",
    "        categorical_columns = []\n",
    "        for column in self.data.columns:\n",
    "            if self.data[column].dtype == 'object':\n",
    "                categorical_columns.append(column)\n",
    "        self.categorical_columns = categorical_columns\n",
    "        # return categorical_columns\n",
    "\n",
    "    def get_categorical_column_cardinality(self):\n",
    "        cardinality = {}\n",
    "        for column in self.categorical_columns:\n",
    "            cardinality[column] = len(self.data[column].unique())\n",
    "        self.cardinality = cardinality\n",
    "\n",
    "    def get_target_encoding_columns(self):\n",
    "        target_encoding_columns = []\n",
    "        for column in self.categorical_columns:\n",
    "            if column != self.target_column and  self.cardinality[column] > self.cardinality_threshold:\n",
    "                target_encoding_columns.append(column)\n",
    "        self.target_encoding_columns = target_encoding_columns\n",
    "    \n",
    "    def get_one_hot_encoding_columns(self):\n",
    "        one_hot_encoding_columns = []\n",
    "        for column in self.categorical_columns:\n",
    "            if column != self.target_column and self.cardinality[column] <= self.cardinality_threshold:\n",
    "                one_hot_encoding_columns.append(column)\n",
    "        self.one_hot_encoding_columns = one_hot_encoding_columns\n",
    "\n",
    "    def encode_target_column(self):\n",
    "        le = LabelEncoder()\n",
    "        self.data[self.target_column] = le.fit_transform(self.data[self.target_column])\n",
    "        self.le = le\n",
    "\n",
    "    def split_data(self):\n",
    "        X = self.data.drop(self.target_column, axis=1)\n",
    "        y = self.data[self.target_column]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.get_numerical_columns()\n",
    "        self.get_categorical_columns()\n",
    "        self.get_categorical_column_cardinality()\n",
    "        self.get_target_encoding_columns()\n",
    "        self.get_one_hot_encoding_columns()\n",
    "        self.split_data()\n",
    "        self.encode_target_column()\n",
    "\n",
    "    def get_preprocessor(self):\n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        target_categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('target', TargetEncoder())\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('numerical', numerical_transformer, self.numerical_columns),\n",
    "                ('one_hot_encoding', categorical_transformer, self.one_hot_encoding_columns),\n",
    "                ('target_encoding', target_categorical_transformer, self.target_encoding_columns)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def get_estimator(self, estimator):\n",
    "\n",
    "        estimator = Pipeline(steps=[\n",
    "            ('preprocessor', self.preprocessor),\n",
    "            ('classifier', estimator)\n",
    "        ])\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def trainAutoML(self):\n",
    "        self.prepare_data()\n",
    "        self.get_preprocessor()\n",
    "        print(\"Status: Setting up AutoML Training\", file=sys.stderr)\n",
    "        classification_metrics = ClassificationMetrics\n",
    "        \n",
    "        results = []\n",
    "        trained_models = {}\n",
    "\n",
    "        pbar = tqdm(self.classifiers)\n",
    "        for classifier in pbar:\n",
    "            self.get_estimator(classifier)\n",
    "            \n",
    "            pbar.set_description(\"Status: %s Current Classifier: %s Processing\" % ('Training', classifier.__class__.__name__))\n",
    "            self.estimator.fit(self.X_train, self.y_train)\n",
    "\n",
    "            trained_models[classifier.__class__.__name__] = self.estimator\n",
    "\n",
    "            y_pred = self.estimator.predict(self.X_test)\n",
    "            # print(y_pred)\n",
    "\n",
    "            accuracy = accuracy_score(self.y_test, y_pred)\n",
    "            precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "            recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "            f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "            if len(self.le.classes_) == 2:\n",
    "                auc = roc_auc_score(self.y_test, self.estimator.predict_proba(self.X_test)[:, 1])\n",
    "            else:\n",
    "                auc = roc_auc_score(self.y_test, self.estimator.predict_proba(self.X_test), average='macro', multi_class='ovo')\n",
    "\n",
    "            results.append({\n",
    "                'classifier' : classifier.__class__.__name__,\n",
    "                classification_metrics.Accuracy.value : round(accuracy, 4),\n",
    "                classification_metrics.Precision.value : round(precision, 4),\n",
    "                classification_metrics.Recall.value : round(recall, 4),\n",
    "                classification_metrics.F1.value : round(f1, 4),\n",
    "                classification_metrics.AUC.value : round(auc, 4)\n",
    "            })\n",
    "        \n",
    "        self.trained_models = trained_models\n",
    "        self.results = pd.DataFrame(results)\n",
    "        self.best_model = self.getBestModel(self.metric_to_optimize)\n",
    "\n",
    "    def trainCustom(self, model_type):\n",
    "        self.prepare_data()\n",
    "        self.get_preprocessor()\n",
    "        print(\"Status: Setting up Custom Training\", file=sys.stderr)\n",
    "        classification_metrics = ClassificationMetrics\n",
    "\n",
    "        results = []\n",
    "        classifier = self.classifiers_dict[model_type]\n",
    "        if self.hyperparameters is not None:\n",
    "            classifier.set_params(**self.hyperparameters)\n",
    "            \n",
    "        self.get_estimator(classifier)\n",
    "\n",
    "        print(\"Status: Started Training \", file=sys.stderr)\n",
    "        self.estimator.fit(self.X_train, self.y_train)\n",
    "        y_pred = self.estimator.predict(self.X_test)\n",
    "        # print(y_pred)\n",
    "\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        if len(self.le.classes_) == 2:\n",
    "            auc = roc_auc_score(self.y_test, self.estimator.predict_proba(self.X_test)[:, 1])\n",
    "        else:\n",
    "            auc = roc_auc_score(self.y_test, self.estimator.predict_proba(self.X_test), average='macro', multi_class='ovo')\n",
    "\n",
    "        results.append({\n",
    "            'classifier' : classifier.__class__.__name__,\n",
    "            classification_metrics.Accuracy.value : round(accuracy, 4),\n",
    "            classification_metrics.Precision.value : round(precision, 4),\n",
    "            classification_metrics.Recall.value : round(recall, 4),\n",
    "            classification_metrics.F1.value : round(f1, 4),\n",
    "            classification_metrics.AUC.value : round(auc, 4)\n",
    "        })\n",
    "\n",
    "        self.results = pd.DataFrame(results)\n",
    "        self.best_model = model_type\n",
    "        self.best_estimator = self.estimator\n",
    "        # self.best_model = self.getBestModel(self.metric_to_optimize)\n",
    "\n",
    "        print(\"Status: Training Completed\", file=sys.stderr)\n",
    "\n",
    "    def getBestModel(self, metric):\n",
    "        self.results.sort_values(by=metric, ascending=False, inplace=True)\n",
    "        self.best_model = self.results.iloc[0]\n",
    "        self.best_estimator = self.trained_models[self.best_model['classifier']]\n",
    "        return self.best_model\n",
    "    \n",
    "    def saveModel(self, model_name, save_path):\n",
    "        joblib.dump(self.trained_models[model_name], save_path)\n",
    "        self.save_path = save_path\n",
    "    \n",
    "    def get_input_schema(self):\n",
    "        self.input_schema = []\n",
    "        for column in self.data.columns:\n",
    "            if column != self.target_column:\n",
    "                self.input_schema.append({\n",
    "                    'column_name' : column,\n",
    "                    'column_type' : self.data[column].dtype.name\n",
    "                })\n",
    "        return self.input_schema\n",
    "    \n",
    "    def get_output_schema(self):\n",
    "        self.output_schema = []\n",
    "        self.output_schema.append({\n",
    "            'column_name' : self.target_column,\n",
    "            'column_type' : self.data[self.target_column].dtype.name\n",
    "        })\n",
    "        return self.output_schema\n",
    "    \n",
    "    def get_output_mapping(self):\n",
    "        self.output_mapping = {}\n",
    "        for i, class_name in enumerate(self.le.classes_):\n",
    "            self.output_mapping[str(i)] = str(class_name)\n",
    "        return self.output_mapping\n",
    "    \n",
    "    def get_confusion_matrix(self):\n",
    "        cm = confusion_matrix(self.y_test, self.best_estimator.predict(self.X_test))\n",
    "        return cm.tolist()\n",
    "    \n",
    "    # def get_learning_curve_data(self):\n",
    "    #     self.get_estimator(self.classifiers_dict[self.best_model['classifier']])\n",
    "    #     train_sizes, train_scores, test_scores = learning_curve(self.estimator, self.X_train, self.y_train, scoring='accuracy', n_jobs=4)\n",
    "    #     learning_curve_data = {}\n",
    "    #     learning_curve_data['train_sizes'] = train_sizes.tolist()\n",
    "    #     learning_curve_data['train_scores'] = train_scores.tolist()\n",
    "    #     learning_curve_data['test_scores'] = test_scores.tolist()\n",
    "    #     return learning_curve_data\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        feature_importance = {}\n",
    "        feature_importance['feature_names'] = self.X_train.columns.tolist()\n",
    "        feature_importance['feature_importance'] = permutation_importance(self.best_estimator, self.X_test, self.y_test, n_repeats=3, random_state=42)['importances_mean'].tolist()\n",
    "        return feature_importance\n",
    "    \n",
    "    def get_precision_recall_data(self):\n",
    "        precision_recall_data = {}\n",
    "        output_mapping = self.get_output_mapping()\n",
    "        # get key corresponding to value 1\n",
    "\n",
    "        pos_label = output_mapping['1']\n",
    "        precision_recall_data['precision'] = []\n",
    "        precision_recall_data['recall'] = []\n",
    "        precision_recall_data['thresholds'] = []\n",
    "        precision_recall_data['auc'] = []\n",
    "        if len(self.output_mapping) == 2:\n",
    "            precision, recall, thresholds = precision_recall_curve(self.y_test, self.best_estimator.predict_proba(self.X_test)[:, 1], pos_label=pos_label)\n",
    "            precision_recall_data['precision'].append(precision.tolist())\n",
    "            precision_recall_data['recall'].append(recall.tolist())\n",
    "            precision_recall_data['thresholds'].append(thresholds.tolist())\n",
    "            precision_recall_data['auc'].append(auc(recall, precision))\n",
    "            return precision_recall_data\n",
    "        else:\n",
    "            for i in range(len(self.output_mapping)):\n",
    "                precision, recall, thresholds = precision_recall_curve(self.y_test, self.best_estimator.predict_proba(self.X_test)[:, i], pos_label=i)\n",
    "                precision_recall_data['precision'].append(precision.tolist())\n",
    "                precision_recall_data['recall'].append(recall.tolist())\n",
    "                precision_recall_data['thresholds'].append(thresholds.tolist())\n",
    "                precision_recall_data['auc'].append(auc(recall, precision))\n",
    "            return precision_recall_data\n",
    "    \n",
    "    def get_auc_data(self):\n",
    "        output_mapping = self.get_output_mapping()\n",
    "        # get key corresponding to value 1\n",
    "\n",
    "        pos_label = output_mapping['1']\n",
    "\n",
    "        auc_data = {}\n",
    "        auc_data['fpr'] = []\n",
    "        auc_data['tpr'] = []\n",
    "        auc_data['thresholds'] = []\n",
    "        auc_data['auc'] = []\n",
    "        if len(output_mapping) == 2:\n",
    "            fpr, tpr, thresholds = roc_curve(self.y_test, self.best_estimator.predict_proba(self.X_test)[:, 1], pos_label=pos_label)\n",
    "            auc_data['fpr'].append(fpr.tolist())\n",
    "            auc_data['tpr'].append(tpr.tolist())\n",
    "            auc_data['thresholds'].append(thresholds.tolist())\n",
    "            auc_data['auc'].append(auc(fpr, tpr))\n",
    "            return auc_data\n",
    "        else:\n",
    "            for i in range(len(output_mapping)):\n",
    "                fpr, tpr, thresholds = roc_curve(self.y_test, self.best_estimator.predict_proba(self.X_test)[:, i], pos_label=i)\n",
    "                auc_data['fpr'].append(fpr.tolist())\n",
    "                auc_data['tpr'].append(tpr.tolist())\n",
    "                auc_data['thresholds'].append(thresholds.tolist())\n",
    "                auc_data['auc'].append(auc(fpr, tpr))\n",
    "            return auc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Status: Setting up Custom Training\n",
      "Status: Started Training \n",
      "Status: Training Completed\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../Datasets/91496.csv')\n",
    "target_column = 'income'\n",
    "clfutil = ClassificationUtility(data, target_column, {'n_estimators': 100, 'learning_rate': 0.05})\n",
    "clfutil.trainCustom('AdaBoostClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           classifier  Accuracy  Precision  Recall      F1     AUC\n",
      "0  AdaBoostClassifier    0.8534     0.8394  0.7238  0.7577  0.9069\n"
     ]
    }
   ],
   "source": [
    "print(clfutil.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 0.05, 'n_estimators': 100, 'random_state': None}\n"
     ]
    }
   ],
   "source": [
    "print(clfutil.best_estimator.named_steps['classifier'].get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'algorithm': 'SAMME', 'n_estimators': 50, 'learning_rate': 0.01}, {'algorithm': 'SAMME', 'n_estimators': 50, 'learning_rate': 0.05}, {'algorithm': 'SAMME', 'n_estimators': 50, 'learning_rate': 0.1}, {'algorithm': 'SAMME', 'n_estimators': 100, 'learning_rate': 0.01}, {'algorithm': 'SAMME', 'n_estimators': 100, 'learning_rate': 0.05}, {'algorithm': 'SAMME', 'n_estimators': 100, 'learning_rate': 0.1}, {'algorithm': 'SAMME', 'n_estimators': 200, 'learning_rate': 0.01}, {'algorithm': 'SAMME', 'n_estimators': 200, 'learning_rate': 0.05}, {'algorithm': 'SAMME', 'n_estimators': 200, 'learning_rate': 0.1}, {'algorithm': 'SAMME.R', 'n_estimators': 50, 'learning_rate': 0.01}, {'algorithm': 'SAMME.R', 'n_estimators': 50, 'learning_rate': 0.05}, {'algorithm': 'SAMME.R', 'n_estimators': 50, 'learning_rate': 0.1}, {'algorithm': 'SAMME.R', 'n_estimators': 100, 'learning_rate': 0.01}, {'algorithm': 'SAMME.R', 'n_estimators': 100, 'learning_rate': 0.05}, {'algorithm': 'SAMME.R', 'n_estimators': 100, 'learning_rate': 0.1}, {'algorithm': 'SAMME.R', 'n_estimators': 200, 'learning_rate': 0.01}, {'algorithm': 'SAMME.R', 'n_estimators': 200, 'learning_rate': 0.05}, {'algorithm': 'SAMME.R', 'n_estimators': 200, 'learning_rate': 0.1}]\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_grid = {\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1]\n",
    "}\n",
    "import itertools\n",
    "hyperparameters = []\n",
    "keys = []\n",
    "for key in hyperparameter_grid:\n",
    "    keys.append(key)\n",
    "    hyperparameters.append(hyperparameter_grid[key])\n",
    "\n",
    "hyperparameters = list(itertools.product(*hyperparameters))\n",
    "\n",
    "hyperparameters_dict = []\n",
    "for hyperparameter in hyperparameters:\n",
    "    hyperparameters_dict.append(dict(zip(keys, hyperparameter)))\n",
    "\n",
    "print(hyperparameters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SAMME', 50, 0.01),\n",
       " ('SAMME', 50, 0.05),\n",
       " ('SAMME', 50, 0.1),\n",
       " ('SAMME', 100, 0.01),\n",
       " ('SAMME', 100, 0.05),\n",
       " ('SAMME', 100, 0.1),\n",
       " ('SAMME', 200, 0.01),\n",
       " ('SAMME', 200, 0.05),\n",
       " ('SAMME', 200, 0.1),\n",
       " ('SAMME.R', 50, 0.01),\n",
       " ('SAMME.R', 50, 0.05),\n",
       " ('SAMME.R', 50, 0.1),\n",
       " ('SAMME.R', 100, 0.01),\n",
       " ('SAMME.R', 100, 0.05),\n",
       " ('SAMME.R', 100, 0.1),\n",
       " ('SAMME.R', 200, 0.01),\n",
       " ('SAMME.R', 200, 0.05),\n",
       " ('SAMME.R', 200, 0.1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
